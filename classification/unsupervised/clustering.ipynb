{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Clustering\n",
    "This notebook clusters the dataset images using the pretrained DinoV2 model. The model is used to extract features from the images, which are then clustered using KMeans."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "from sklearn.cluster import KMeans\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np  \n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 224\n",
    "datapath = r\"D:\\Database\\animals\\dataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14', pretrained=True)\n",
    "model.cuda()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(int(img_size * 1.1)),\n",
    "    transforms.CenterCrop(img_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406], \n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "# Iterate over all images and extract features\n",
    "outputs = []\n",
    "for root, dirs, files in os.walk(datapath):\n",
    "    for p in tqdm(files):\n",
    "        if not p.endswith('.jpg'):\n",
    "            continue\n",
    "        img = Image.open(os.path.join(root, p))\n",
    "        img = transform(img).unsqueeze(0).cuda()\n",
    "        with torch.no_grad():\n",
    "            output = model(img)\n",
    "            output = output.squeeze(0)\n",
    "            output = output.cpu().numpy()\n",
    "            outputs.append(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Many Clusters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmax = 250\n",
    "sil = []\n",
    "K = range(2, kmax+1)\n",
    "for k in tqdm(K, desc='Finding best k', unit='k'):\n",
    "  kmeans = KMeans(n_clusters = k, n_init='auto').fit(outputs)\n",
    "  labels = kmeans.labels_\n",
    "  sil.append(silhouette_score(outputs, labels, metric = 'cosine'))\n",
    "\n",
    "\n",
    "best_k = sil.index(max(sil)) + 2 # offset due to k in [2, 250]\n",
    "best_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the silhouette scores\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(K, sil)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KMeans Clustering with best `k`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=best_k, n_init='auto')\n",
    "labels = kmeans.fit_predict(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Clusters with t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D t-SNE plot\n",
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "X = tsne.fit_transform(np.array(outputs))\n",
    "df = pd.DataFrame(X, columns=['x', 'y'])\n",
    "df['label'] = labels\n",
    "\n",
    "fig = px.scatter(df, x='x', y='y', color='label', color_discrete_sequence=px.colors.qualitative.G10)\n",
    "fig.update_layout(scene=dict(xaxis_title='X', yaxis_title='Y'), showlegend=False,    xaxis=dict(visible=False), yaxis=dict(visible=False),)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D t-SNE plot\n",
    "tsne = TSNE(n_components=3, random_state=0)\n",
    "X = tsne.fit_transform(np.array(outputs))\n",
    "df = pd.DataFrame(X, columns=['x', 'y', 'z'])\n",
    "df['label'] = labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "fig = px.scatter_3d(df, x='x', y='y', z='z', color='label', color_discrete_map=\"identity\", template='plotly_white')\n",
    "fig.update_traces(marker=dict(size=2))\n",
    "fig.update_layout(scene = dict(showlegend=False, xaxis=dict(visible=False), yaxis=dict(visible=False),\n",
    "        ))\n",
    "\n",
    "x_eye = -1.25\n",
    "y_eye = 2\n",
    "z_eye = 0.5\n",
    "fig.update_layout(\n",
    "         title='Animation Test',\n",
    "         width=1000,\n",
    "         height=1000,\n",
    "         scene_camera_eye=dict(x=x_eye, y=y_eye, z=z_eye),\n",
    "         updatemenus=[dict(type='buttons',\n",
    "                  showactive=False,\n",
    "                  y=1,\n",
    "                  x=0.8,\n",
    "                  xanchor='left',\n",
    "                  yanchor='bottom',\n",
    "                  pad=dict(t=45, r=10),\n",
    "                  buttons=[dict(label='Play',\n",
    "                                 method='animate',\n",
    "                                 args=[None, dict(frame=dict(duration=5, redraw=True), \n",
    "                                                             transition=dict(duration=0),\n",
    "                                                             fromcurrent=True,\n",
    "                                                             mode='immediate'\n",
    "                                                            )]\n",
    "                                            )\n",
    "                                      ]\n",
    "                              )\n",
    "                        ]\n",
    ")\n",
    "\n",
    "def rotate_z(x, y, z, theta):\n",
    "    w = x+1j*y\n",
    "    return np.real(np.exp(1j*theta)*w), np.imag(np.exp(1j*theta)*w), z\n",
    "\n",
    "frames=[]\n",
    "for t in np.arange(0, 6.26, 0.1):\n",
    "    xe, ye, ze = rotate_z(x_eye, y_eye, z_eye, -t)\n",
    "    frames.append(go.Frame(layout=dict(scene_camera_eye=dict(x=xe, y=ye, z=ze))))\n",
    "fig.frames=frames\n",
    "plt.axis('off')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concluding Notes\n",
    "The best determined number of clusters is deviating from the number of classes in the dataset (90). This could be due to the fact that the DinoV2 model was trained on a different dataset, which might have different features. The model might not be able to extract the features that are needed to cluster the images by their classes. Furthermore, there could be unsuitable samples in the dataset, which are not representative for their class. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
